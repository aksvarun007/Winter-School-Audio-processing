<?xml version="1.0" encoding="ascii" ?>

<script language="javascript" type="text/javascript">

function astext(node)
{
    return node.innerHTML.replace(/(<([^>]+)>)/ig,"")
                         .replace(/&gt;/ig, ">")
                         .replace(/&lt;/ig, "<")
                         .replace(/&quot;/ig, '"')
                         .replace(/&amp;/ig, "&");
}

function copy_notify(node, bar_color, data)
{
    // The outer box: relative + inline positioning.
    var box1 = document.createElement("div");
    box1.style.position = "relative";
    box1.style.display = "inline";
    box1.style.top = "2em";
    box1.style.left = "1em";
  
    // A shadow for fun
    var shadow = document.createElement("div");
    shadow.style.position = "absolute";
    shadow.style.left = "-1.3em";
    shadow.style.top = "-1.3em";
    shadow.style.background = "#404040";
    
    // The inner box: absolute positioning.
    var box2 = document.createElement("div");
    box2.style.position = "relative";
    box2.style.border = "1px solid #a0a0a0";
    box2.style.left = "-.2em";
    box2.style.top = "-.2em";
    box2.style.background = "white";
    box2.style.padding = ".3em .4em .3em .4em";
    box2.style.fontStyle = "normal";
    box2.style.background = "#f0e0e0";

    node.insertBefore(box1, node.childNodes.item(0));
    box1.appendChild(shadow);
    shadow.appendChild(box2);
    box2.innerHTML="Copied&nbsp;to&nbsp;the&nbsp;clipboard: " +
                   "<pre class='copy-notify'>"+
                   data+"</pre>";
    setTimeout(function() { node.removeChild(box1); }, 1000);

    var elt = node.parentNode.firstChild;
    elt.style.background = "#ffc0c0";
    setTimeout(function() { elt.style.background = bar_color; }, 200);
}

function copy_codeblock_to_clipboard(node)
{
    var data = astext(node)+"\n";
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#40a060", data);
    }
}

function copy_doctest_to_clipboard(node)
{
    var s = astext(node)+"\n   ";
    var data = "";

    var start = 0;
    var end = s.indexOf("\n");
    while (end >= 0) {
        if (s.substring(start, start+4) == ">>> ") {
            data += s.substring(start+4, end+1);
        }
        else if (s.substring(start, start+4) == "... ") {
            data += s.substring(start+4, end+1);
        }
        /*
        else if (end-start > 1) {
            data += "# " + s.substring(start, end+1);
        }*/
        // Grab the next line.
        start = end+1;
        end = s.indexOf("\n", start);
    }
    
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#4060a0", data);
    }
}
    
function copy_text_to_clipboard(data)
{
    if (window.clipboardData) {
        window.clipboardData.setData("Text", data);
        return true;
     }
    else if (window.netscape) {
        // w/ default firefox settings, permission will be denied for this:
        netscape.security.PrivilegeManager
                      .enablePrivilege("UniversalXPConnect");
    
        var clip = Components.classes["@mozilla.org/widget/clipboard;1"]
                      .createInstance(Components.interfaces.nsIClipboard);
        if (!clip) return;
    
        var trans = Components.classes["@mozilla.org/widget/transferable;1"]
                       .createInstance(Components.interfaces.nsITransferable);
        if (!trans) return;
    
        trans.addDataFlavor("text/unicode");
    
        var str = new Object();
        var len = new Object();
    
        var str = Components.classes["@mozilla.org/supports-string;1"]
                     .createInstance(Components.interfaces.nsISupportsString);
        var datacopy=data;
        str.data=datacopy;
        trans.setTransferData("text/unicode",str,datacopy.length*2);
        var clipid=Components.interfaces.nsIClipboard;
    
        if (!clip) return false;
    
        clip.setData(trans,null,clipid.kGlobalClipboard);
        return true;
    }
    return false;
}
//-->
</script>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ascii" />
<meta name="generator" content="Docutils 0.12: http://docutils.sourceforge.net/" />
<title>8. Analyzing Sentence Structure</title>
<style type="text/css">

/*
:Author: Edward Loper, James Curran
:Copyright: This stylesheet has been placed in the public domain.

Stylesheet for use with Docutils.

This stylesheet defines new css classes used by NLTK.

It uses a Python syntax highlighting scheme that matches
the colour scheme used by IDLE, which makes it easier for
beginners to check they are typing things in correctly.
*/

/* Include the standard docutils stylesheet. */
@import url(default.css);

/* Custom inline roles */
span.placeholder    { font-style: italic; font-family: monospace; }
span.example        { font-style: italic; }
span.emphasis       { font-style: italic; }
span.termdef        { font-weight: bold; }
/*span.term           { font-style: italic; }*/
span.category       { font-variant: small-caps; }
span.feature        { font-variant: small-caps; }
span.fval           { font-style: italic; }
span.math           { font-style: italic; }
span.mathit         { font-style: italic; }
span.lex            { font-variant: small-caps; }
span.guide-linecount{ text-align: right; display: block;}

/* Python souce code listings */
span.pysrc-prompt   { color: #9b0000; }
span.pysrc-more     { color: #9b00ff; }
span.pysrc-keyword  { color: #e06000; }
span.pysrc-builtin  { color: #940094; }
span.pysrc-string   { color: #00aa00; }
span.pysrc-comment  { color: #ff0000; }
span.pysrc-output   { color: #0000ff; }
span.pysrc-except   { color: #ff0000; }
span.pysrc-defname  { color: #008080; }


/* Doctest blocks */
pre.doctest         { margin: 0; padding: 0; font-weight: bold; }
div.doctest         { margin: 0 1em 1em 1em; padding: 0; }
table.doctest       { margin: 0; padding: 0;
                      border-top: 1px solid gray;
                      border-bottom: 1px solid gray; }
pre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;
                      background-color: #ffffff; }

/* Python source listings */
div.pylisting       { margin: 0 1em 1em 1em; padding: 0; }
table.pylisting     { margin: 0; padding: 0;
                      border-top: 1px solid gray; }
td.caption { border-top: 1px solid black; margin: 0; padding: 0; }
.caption-label { font-weight: bold;  }
td.caption p { margin: 0; padding: 0; font-style: normal;}

table tr td.codeblock { 
  padding: 0.2em ! important; margin: 0;
  border-left: 1px solid gray;
  border-right: 2px solid gray;
  border-top: 0px solid gray;
  border-bottom: 1px solid gray;
  font-weight: bold; background-color: #eeffee;
}

table tr td.doctest  { 
  padding: 0.2em; margin: 0;
  border-left: 1px solid gray;
  border-right: 2px solid gray;
  border-top: 0px solid gray;
  border-bottom: 1px solid gray;
  font-weight: bold; background-color: #eeeeff;
}

td.codeblock table tr td.copybar {
    background: #40a060; border: 1px solid gray;
    font-family: monospace; padding: 0; margin: 0; }
td.doctest table tr td.copybar {
    background: #4060a0; border: 1px solid gray;
    font-family: monospace; padding: 0; margin: 0; }

td.pysrc { padding-left: 0.5em; }

img.callout { border-width: 0px; }

table.docutils {
    border-style: solid;
    border-width: 1px;
    margin-top: 6px;
    border-color: grey;
    border-collapse: collapse; }

table.docutils th {
    border-style: none;
    border-width: 1px;
    border-color: grey;
    padding: 0 .5em 0 .5em; }

table.docutils td {
    border-style: none;
    border-width: 1px;
    border-color: grey; 
    padding: 0 .5em 0 .5em; }

table.footnote td { padding: 0; }
table.footnote { border-width: 0; }
table.footnote td { border-width: 0; }
table.footnote th { border-width: 0; }

table.noborder { border-width: 0; }

table.example pre { margin-top: 4px; margin-bottom: 0; }

/* For figures & tables */
p.caption { margin-bottom: 0; }
div.figure { text-align: center; }

/* The index */
div.index { border: 1px solid black;
            background-color: #eeeeee; }
div.index h1 { padding-left: 0.5em; margin-top: 0.5ex;
               border-bottom: 1px solid black; }
ul.index { margin-left: 0.5em; padding-left: 0; }
li.index { list-style-type: none; }
p.index-heading { font-size: 120%; font-style: italic; margin: 0; }
li.index ul { margin-left: 2em; padding-left: 0; }

/* 'Note' callouts */
div.note
{
  border-right:   #87ceeb 1px solid;
  padding-right: 4px;
  border-top: #87ceeb 1px solid;
  padding-left: 4px;
  padding-bottom: 4px;
  margin: 2px 5% 10px;
  border-left: #87ceeb 1px solid;
  padding-top: 4px;
  border-bottom: #87ceeb 1px solid;
  font-style: normal;
  font-family: verdana, arial;
  background-color: #b0c4de;
}

table.avm { border: 0px solid black; width: 0; }
table.avm tbody tr {border: 0px solid black; }
table.avm tbody tr td { padding: 2px; }
table.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }
table.avm tbody tr td.avm-eq { padding: 5px; }
table.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }
p.avm-empty { font-style: normal; }
table.avm colgroup col { border: 0px solid black; }
table.avm tbody tr td.avm-topleft 
    { border-left: 2px solid #000080; border-top: 2px solid #000080; }
table.avm tbody tr td.avm-botleft 
    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }
table.avm tbody tr td.avm-topright
    { border-right: 2px solid #000080; border-top: 2px solid #000080; }
table.avm tbody tr td.avm-botright
    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }
table.avm tbody tr td.avm-left
    { border-left: 2px solid #000080; }
table.avm tbody tr td.avm-right
    { border-right: 2px solid #000080; }
table.avm tbody tr td.avm-topbotleft
    { border: 2px solid #000080; border-right: 0px solid black; }
table.avm tbody tr td.avm-topbotright
    { border: 2px solid #000080; border-left: 0px solid black; }
table.avm tbody tr td.avm-ident
    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }
.avm-pointer
{ border: 1px solid #008000; padding: 1px; color: #008000; 
  background: #c0ffc0; font-style: normal; }

table.gloss { border: 0px solid black; width: 0; }
table.gloss tbody tr { border: 0px solid black; }
table.gloss tbody tr td { border: 0px solid black; }
table.gloss colgroup col { border: 0px solid black; }
table.gloss p { margin: 0; padding: 0; }

table.rst-example { border: 1px solid black; }
table.rst-example tbody tr td { background: #eeeeee; }
table.rst-example thead tr th { background: #c0ffff; }
td.rst-raw { width: 0; }

/* Used by nltk.org/doc/test: */
div.doctest-list { text-align: center; }
table.doctest-list { border: 1px solid black;
  margin-left: auto; margin-right: auto;
}
table.doctest-list tbody tr td { background: #eeeeee;
  border: 1px solid #cccccc; text-align: left; }
table.doctest-list thead tr th { background: #304050; color: #ffffff;
  border: 1px solid #000000;}
table.doctest-list thead tr a { color: #ffffff; }
span.doctest-passed { color: #008000; }
span.doctest-failed { color: #800000; }

</style>
</head>
<body>
<div class="document" id="analyzing-sentence-structure">
<span id="chap-parse"></span>
<h1 class="title">8. Analyzing Sentence Structure</h1>

<!-- -*- mode: rst -*- -->
<!-- -*- mode: rst -*- -->
<!-- CAP abbreviations (map to small caps in LaTeX) -->
<!-- Other candidates for global consistency -->
<!-- PTB removed since it must be indexed -->
<!-- WN removed since it must be indexed -->
<!-- misc & punctuation -->
<!-- cdots was unicode U+22EF but not working -->
<!-- exercise meta-tags -->
<!-- Unicode tests -->
<!-- phonetic -->
<!-- misc -->
<!-- used in Unicode section -->
<!-- arrows -->
<!-- unification stuff -->
<!-- Math & Logic -->
<!-- sets -->
<!-- Greek -->
<!-- Chinese -->
<!-- URLs -->
<!-- Python example - a snippet of code in running text -->
<!-- PlaceHolder example -  something that should be replaced by actual code -->
<!-- Linguistic eXample - cited form in running text -->
<!-- Emphasized (more declarative than just using *) -->
<!-- Grammatical Category - e.g. NP and verb as technical terms
.. role:: gc
   :class: category -->
<!-- Math expression - e.g. especially for variables -->
<!-- Textual Math expression - for words 'inside' a math environment -->
<!-- Feature (or attribute) -->
<!-- Raw LaTeX -->
<!-- Raw HTML -->
<!-- Feature-value -->
<!-- Lexemes -->
<!-- Replacements that rely on previous definitions :-) -->
<!-- standard global imports

>>> from __future__ import division
>>> import nltk, re, pprint -->
<!-- TODO: include overview of ContextFreeGrammar and Production in 8.4 -->
<!-- TODO: make URLs clickable in the HTML version -->
<!-- TODO: mention Chomsky Normal Form -->
<!-- TODO: give an example of a text generated from bigrams, then talk the
reader through constructing a simple grammar over this by talking
about local contexts, and contrast this with the grammar -->
<!-- TODO: update images in parser-problem table (NP -> NP PP replaced
by Nom -> Nom PP -->
<!-- TODO: find a different example of left-recursive rules -->
<!-- TODO: Add discussion of non-projective dependency parsing -->
<p>Earlier chapters focused on words: how to identify them,
analyze their structure, assign them to lexical categories,
and access their meanings.
We have also seen how to identify patterns in word sequences or n-grams.
However, these methods only scratch the surface of the complex constraints
that govern sentences.
We need a way to deal with the ambiguity that natural language is famous for.
We also need to be able to cope with the fact that there are an unlimited number
of possible sentences, and we can only write finite programs to analyze their
structures and discover their meanings.</p>
<p>The goal of this chapter is to answer the following questions:</p>
<ol class="arabic simple">
<li>How can we use a formal grammar to describe the structure of an unlimited set of sentences?</li>
<li>How do we represent the structure of sentences using syntax trees?</li>
<li>How do parsers analyze a sentence and automatically build a syntax tree?</li>
</ol>
<p>Along the way, we will cover the fundamentals of English syntax, and
see that there are systematic aspects of meaning that are much easier
to capture once we have identified the structure of sentences.</p>
<div class="section" id="some-grammatical-dilemmas">
<span id="sec-dilemmas"></span><h1>1&nbsp;&nbsp;&nbsp;Some Grammatical Dilemmas</h1>
<div class="section" id="linguistic-data-and-unlimited-possibilities">
<h2>1.1&nbsp;&nbsp;&nbsp;Linguistic Data and Unlimited Possibilities</h2>
<p>Previous chapters have shown you how to process and analyse text
corpora, and we have stressed the challenges for NLP in dealing with
the vast amount of electronic language data that is growing
daily. Let's consider this data more closely, and make the thought
experiment that we have a gigantic corpus consisting of everything
that has been either uttered or written in English over, say, the last
50 years. Would we be justified in calling this corpus &quot;the language
of modern English&quot;? There are a number of reasons why we might answer
No. Recall that in <a class="reference external" href="ch03.html#chap-words">3</a>, we asked you to search
the web for instances of the pattern <span class="example">the of</span>.  Although it is
easy to find examples on the web containing this word sequence, such as
<span class="example">New man at the of IMG</span>
(<tt class="doctest"><span class="pre">http://www.telegraph.co.uk/sport/2387900/New-man-at-the-of-IMG.html</span></tt>),
speakers of English will say that most such examples are errors, and
therefore not part of English after all.</p>
<p>Accordingly, we can argue
that the &quot;modern English&quot; is not equivalent to the very big
set of word sequences in our imaginary corpus. Speakers
of English can make judgements about these sequences, and will reject
some of them as being ungrammatical.</p>
<p>Equally, it is easy to compose a new sentence and have speakers agree that it is perfectly
good English.  For example, sentences have an interesting property
that they can be embedded inside larger sentences.  Consider the
following sentences:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(1)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>Usain Bolt broke the 100m record</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>The Jamaica Observer reported that Usain Bolt broke the 100m record</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td>Andre said The Jamaica Observer reported that Usain Bolt broke the 100m record</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">d.</td><td width="15"></td><td>I think Andre said the Jamaica Observer reported that Usain Bolt broke the 100m record</td></tr></table></p>
</td></tr></table></p>
<p>If we replaced whole sentences with the symbol <tt class="doctest"><span class="pre">S</span></tt>, we would see patterns like
<span class="example">Andre said</span> <tt class="doctest"><span class="pre">S</span></tt> and <span class="example">I think</span> <tt class="doctest"><span class="pre">S</span></tt>.  These are templates for taking a sentence
and constructing a bigger sentence.  There are other templates we can use, like
<tt class="doctest"><span class="pre">S</span></tt> <span class="example">but</span> <tt class="doctest"><span class="pre">S</span></tt>, and <tt class="doctest"><span class="pre">S</span></tt> <span class="example">when</span> <tt class="doctest"><span class="pre">S</span></tt>.  With a bit of ingenuity we can
construct some really long sentences using these templates.
Here's an impressive example from a Winnie the Pooh story by A.A. Milne,
<em>In which Piglet is Entirely Surrounded by Water</em>:</p>
<blockquote>
[You can imagine Piglet's joy when at last the ship came in sight of
him.] In after-years he liked to think that he had been in Very
Great Danger during the Terrible Flood, but the only danger he had
really been in was the last half-hour of his imprisonment, when
Owl, who had just flown up, sat on a branch of his tree to comfort
him, and told him a very long story about an aunt who had once laid
a seagull's egg by mistake, and the story went on and on, rather
like this sentence, until Piglet who was listening out of his
window without much hope, went to sleep quietly and naturally,
slipping slowly out of the window towards the water until he was
only hanging on by his toes, at which moment, luckily, a sudden
loud squawk from Owl, which was really part of the story, being
what his aunt said, woke the Piglet up and just gave him time to
jerk himself back into safety and say, &quot;How interesting, and did
she?&quot; when &#8212; well, you can imagine his joy when at last he saw
the good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear)
coming over the sea to rescue him...</blockquote>
<p>This long sentence actually has a simple structure that begins
<span class="example">S but S when S</span>.  We can see from this example that language
provides us with constructions which seem to allow us to extend
sentences indefinitely.  It is also striking that
we can understand sentences of arbitrary length
that we've never heard before:  it's not hard to concoct an
entirely novel sentence, one that has probably never been used before
in the history of the language, yet all speakers of the language
will understand it.</p>
<p>The purpose of a grammar is to give an explicit description of a
language. But the way in which we think of a grammar is closely
intertwined with what we consider to be a language. Is it a
large but finite set of observed utterances and written texts? Is it
something more abstract like the implicit knowledge that competent
speakers have about grammatical sentences? Or is it some combination
of the two? We won't take a stand on this issue, but instead will
introduce the main approaches.</p>
<p>In this chapter, we will adopt the formal framework
of &quot;generative grammar&quot;, in which
a &quot;language&quot; is considered to be nothing more than an
enormous collection of all grammatical sentences, and a
grammar is a formal notation that can be used for &quot;generating&quot; the
members of this set.  Grammars use recursive <a name="productions_index_term" /><span class="termdef">productions</span>
of the form <tt class="doctest"><span class="pre">S</span></tt> &#8594; <tt class="doctest"><span class="pre">S</span></tt> <span class="example">and</span> <tt class="doctest"><span class="pre">S</span></tt>, as we will explore in
<a class="reference internal" href="#sec-context-free-grammar">3</a>.  In <a class="reference external" href="ch10.html#chap-semantics">10.</a> we will extend this,
to automatically build up the meaning of a sentence out of the meanings
of its parts.</p>
</div>
<div class="section" id="ubiquitous-ambiguity">
<h2>1.2&nbsp;&nbsp;&nbsp;Ubiquitous Ambiguity</h2>
<p>A well-known example of ambiguity is shown in <a class="reference internal" href="#ex-marx-elephant">(2)</a>,
from the Groucho Marx movie, <em>Animal Crackers</em> (1930):</p>
<!-- http://www.youtube.com/watch?v=NfN_gcjGoJo -->
<span class="target" id="ex-marx-elephant"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(2)</td><td width="15"></td><td>While hunting in Africa, I shot an elephant in my pajamas.
How he got into my pajamas, I don't know.</td></tr></table></p>
<p>Let's take a closer look at the ambiguity in the phrase:
<span class="example">I shot an elephant in my pajamas</span>.  First we
need to define a simple grammar:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>groucho_grammar = nltk.CFG.fromstring(<span class="pysrc-string">&quot;&quot;&quot;</span>
<span class="pysrc-more">... </span><span class="pysrc-string">S -&gt; NP VP</span>
<span class="pysrc-more">... </span><span class="pysrc-string">PP -&gt; P NP</span>
<span class="pysrc-more">... </span><span class="pysrc-string">NP -&gt; Det N | Det N PP | 'I'</span>
<span class="pysrc-more">... </span><span class="pysrc-string">VP -&gt; V NP | VP PP</span>
<span class="pysrc-more">... </span><span class="pysrc-string">Det -&gt; 'an' | 'my'</span>
<span class="pysrc-more">... </span><span class="pysrc-string">N -&gt; 'elephant' | 'pajamas'</span>
<span class="pysrc-more">... </span><span class="pysrc-string">V -&gt; 'shot'</span>
<span class="pysrc-more">... </span><span class="pysrc-string">P -&gt; 'in'</span>
<span class="pysrc-more">... </span><span class="pysrc-string">&quot;&quot;&quot;</span>)</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>This grammar permits the sentence to be analyzed in two ways,
depending on whether the prepositional phrase <span class="example">in my pajamas</span>
describes the elephant or the shooting event.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sent = [<span class="pysrc-string">'I'</span>, <span class="pysrc-string">'shot'</span>, <span class="pysrc-string">'an'</span>, <span class="pysrc-string">'elephant'</span>, <span class="pysrc-string">'in'</span>, <span class="pysrc-string">'my'</span>, <span class="pysrc-string">'pajamas'</span>]
<span class="pysrc-prompt">&gt;&gt;&gt; </span>parser = nltk.ChartParser(groucho_grammar)
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">for</span> tree <span class="pysrc-keyword">in</span> parser.parse(sent):
<span class="pysrc-more">... </span>    <span class="pysrc-keyword">print</span>(tree)
<span class="pysrc-more">...</span>
<span class="pysrc-output">(S</span>
<span class="pysrc-output">  (NP I)</span>
<span class="pysrc-output">  (VP</span>
<span class="pysrc-output">    (VP (V shot) (NP (Det an) (N elephant)))</span>
<span class="pysrc-output">    (PP (P in) (NP (Det my) (N pajamas)))))</span>
<span class="pysrc-output">(S</span>
<span class="pysrc-output">  (NP I)</span>
<span class="pysrc-output">  (VP</span>
<span class="pysrc-output">    (V shot)</span>
<span class="pysrc-output">    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>The program produces two bracketed structures, which we can depict as
trees, as shown in <a class="reference internal" href="#ex-elephant">(3b)</a>:</p>
<span class="target" id="ex-elephant"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(3)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td><img alt="tree_images/ch08-tree-1.png" class="align-top" src="tree_images/ch08-tree-1.png" style="width: 346.0px; height: 225.0px;" /></td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td><img alt="tree_images/ch08-tree-2.png" class="align-top" src="tree_images/ch08-tree-2.png" style="width: 347.0px; height: 265.0px;" /></td></tr></table></p>
</td></tr></table></p>
<p>Notice that there's no ambiguity concerning the meaning of any of the words;
e.g. the word <span class="example">shot</span> doesn't refer to the act of using a gun in the first sentence,
and using a camera in the second sentence.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Consider the following sentences and see if you can think of two quite different
interpretations: <span class="example">Fighting animals could be dangerous.</span>
<span class="example">Visiting relatives can be tiresome.</span>  Is ambiguity of the individual
words to blame?  If not, what is the cause of the ambiguity?</p>
</div>
<p>This chapter presents grammars and parsing, as the formal and
computational methods for investigating and modeling the linguistic
phenomena we have been discussing.
As we shall see, patterns of well-formedness and ill-formedness in a
sequence of words can be understood with respect to the
phrase structure and dependencies.  We can develop formal
models of these structures using grammars and parsers.
As before, a key motivation is natural language <em>understanding</em>.  How
much more of the meaning of a text can we access when we can reliably
recognize the linguistic structures it contains?  Having read in a
text, can a program &quot;understand&quot; it enough to be able to answer simple
questions about &quot;what happened&quot; or &quot;who did what to whom&quot;?  Also as
before, we will develop simple programs to process annotated corpora
and perform useful tasks.</p>
</div>
</div>
<div class="section" id="what-s-the-use-of-syntax">
<span id="sec-whats-the-use-of-syntax"></span><h1>2&nbsp;&nbsp;&nbsp;What's the Use of Syntax?</h1>
<div class="section" id="beyond-n-grams">
<h2>2.1&nbsp;&nbsp;&nbsp;Beyond n-grams</h2>
<p>We gave an example in <a class="reference external" href="ch02.html#chap-corpora">2.</a> of how to use
the frequency information in bigrams to generate text that seems
perfectly acceptable for small sequences of words but rapidly
degenerates into nonsense. Here's another pair of examples that we created by
computing the bigrams over the text of a childrens' story, <em>The
Adventures of Buster Brown</em> (<tt class="doctest"><span class="pre">http://www.gutenberg.org/files/22816/22816.txt</span></tt>):</p>
<span class="target" id="ex-salad"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(4)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>He roared with me the pail slip down his back</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>The worst part and clumsy looking for whoever heard light</td></tr></table></p>
</td></tr></table></p>
<p>You intuitively know that these sequences are &quot;word-salad&quot;, but you
probably find it hard to pin down what's wrong with them. One
benefit of studying grammar is that it provides a conceptual framework
and vocabulary for spelling out these intuitions. Let's take a closer look
at the sequence <span class="example">the worst part and clumsy looking</span>. This looks like a <a name="coordinate_structure_index_term" /><span class="termdef">coordinate
structure</span>, where two phrases are joined by a coordinating
conjunction such as <span class="example">and</span>, <span class="example">but</span> or <span class="example">or</span>. Here's an
informal (and simplified) statement of how coordination works
syntactically:</p>
<p>Coordinate Structure:</p>
<blockquote>
If <em>v</em><sub>1</sub> and <em>v</em><sub>2</sub> are both phrases of grammatical
category <em>X</em>, then <em>v</em><sub>1</sub> <span class="example">and</span> <em>v</em><sub>2</sub> is also a
phrase of category  <em>X</em>.</blockquote>
<p>Here are a couple of examples. In the first, two <tt class="doctest"><span class="pre">NP</span></tt>s (noun
phrases) have been conjoined to make an <tt class="doctest"><span class="pre">NP</span></tt>, while in the second,
two <tt class="doctest"><span class="pre">AP</span></tt>s (adjective phrases) have been conjoined to make an
<tt class="doctest"><span class="pre">AP</span></tt>.</p>
<span class="target" id="ex-coord"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(5)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>The book's ending was (NP <em>the worst part and the best part</em>) for me.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>On land they are (AP <em>slow and clumsy looking</em>).</td></tr></table></p>
</td></tr></table></p>
<p>What we <span class="emphasis">can't</span> do is conjoin an <tt class="doctest"><span class="pre">NP</span></tt> and an <tt class="doctest"><span class="pre">AP</span></tt>, which is
why <span class="example">the worst part and clumsy looking</span> is ungrammatical.
Before we can formalize these ideas, we need to
understand the concept of <a name="constituent_structure_index_term" /><span class="termdef">constituent structure</span>.</p>
<p>Constituent structure is based on the observation that words combine
with other words to form units. The evidence that a sequence of words
forms such a unit is given by substitutability &#8212; that is, a
sequence of words in a well-formed sentence can be replaced by a
shorter sequence without rendering the sentence ill-formed. To clarify
this idea, consider the following sentence:</p>
<span class="target" id="ex-bb0"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(6)</td><td width="15"></td><td>The little bear saw the fine fat trout in the brook.</td></tr></table></p>
<p>The fact that we can substitute <span class="example">He</span> for <span class="example">The little bear</span>
indicates that the latter sequence is a unit. By contrast, we cannot
replace  <span class="example">little bear saw</span> in the same way.</p>
<!-- explain * ? -->
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(7)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>He saw the fine fat trout in the brook.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>*The he the fine fat trout in the brook.</td></tr></table></p>
</td></tr></table></p>
<p>In <a class="reference internal" href="#fig-ic-diagram">2.1</a>, we systematically substitute longer sequences
by shorter ones in a way which preserves grammaticality. Each sequence
that forms a unit can in fact be replaced by a single word, and we end
up with just two elements.</p>
<span class="target" id="fig-ic-diagram"></span><div class="figure" id="fig-ic-diagram">
<img alt="../images/ic_diagram.png" src="../images/ic_diagram.png" style="width: 463.75px; height: 152.25px;" />
<p class="caption"><span class="caption-label">Figure 2.1</span>: Substitution of Word Sequences: working from the top row, we can replace
particular sequences of words (e.g. <span class="example">the brook</span>) with individual
words (e.g. <span class="example">it</span>); repeating this process we arrive at a grammatical
two-word sentence.</p>
</div>
<p>In <a class="reference internal" href="#fig-ic-diagram-labeled">2.2</a>, we have added
grammatical category labels to the words we saw in the earlier figure.
The labels <tt class="doctest"><span class="pre">NP</span></tt>, <tt class="doctest"><span class="pre">VP</span></tt>, and <tt class="doctest"><span class="pre">PP</span></tt> stand for <a name="noun_phrase_index_term" /><span class="termdef">noun phrase</span>,
<a name="verb_phrase_index_term" /><span class="termdef">verb phrase</span> and <a name="prepositional_phrase_index_term" /><span class="termdef">prepositional phrase</span> respectively.</p>
<span class="target" id="fig-ic-diagram-labeled"></span><div class="figure" id="fig-ic-diagram-labeled">
<img alt="../images/ic_diagram_labeled.png" src="../images/ic_diagram_labeled.png" style="width: 463.75px; height: 152.25px;" />
<p class="caption"><span class="caption-label">Figure 2.2</span>: Substitution of Word Sequences Plus Grammatical Categories:
This diagram reproduces <a class="reference internal" href="#fig-ic-diagram">2.1</a> along with grammatical
categories corresponding to noun phrases (<tt class="doctest"><span class="pre">NP</span></tt>), verb phrases (<tt class="doctest"><span class="pre">VP</span></tt>),
prepositional phrases (<tt class="doctest"><span class="pre">PP</span></tt>), and nominals (<tt class="doctest"><span class="pre">Nom</span></tt>).</p>
</div>
<p>If we now strip out the words apart from the topmost row, add an
<tt class="doctest"><span class="pre">S</span></tt> node, and flip the figure over, we end up with a standard
phrase structure tree, shown in <a class="reference internal" href="#ex-phrase-structure-tree">(8)</a>.
Each node in this tree (including the words) is called
a <a name="constituent_index_term" /><span class="termdef">constituent</span>.  The <a name="immediate_constituents_index_term" /><span class="termdef">immediate constituents</span> of
<tt class="doctest"><span class="pre">S</span></tt> are <tt class="doctest"><span class="pre">NP</span></tt> and <tt class="doctest"><span class="pre">VP</span></tt>.</p>
<span class="target" id="ex-phrase-structure-tree"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(8)</td><td width="15"></td><td><img alt="tree_images/ch08-tree-3.png" class="align-top" src="tree_images/ch08-tree-3.png" style="width: 479.0px; height: 265.0px;" /></td></tr></table></p>
<p>As we will see in the next section, a grammar specifies how the sentence
can be subdivided into its immediate constituents, and how these can be further
subdivided until we reach the level of individual words.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">As we saw in <a class="reference internal" href="#sec-dilemmas">1</a>, sentences can have arbitrary length.
Consequently, phrase structure trees can have arbitrary <span class="emphasis">depth</span>.
The cascaded chunk parsers we saw in <a class="reference external" href="ch07.html#sec-recursion-in-linguistic-structure">4</a>
can only produce structures of bounded depth, so chunking methods
aren't applicable here.</p>
</div>
</div>
</div>
<div class="section" id="context-free-grammar">
<span id="sec-context-free-grammar"></span><h1>3&nbsp;&nbsp;&nbsp;Context Free Grammar</h1>
<div class="section" id="a-simple-grammar">
<h2>3.1&nbsp;&nbsp;&nbsp;A Simple Grammar</h2>
<!-- XXX say more about what "admitted by a grammar" means? -->
<p>Let's start off by looking at a simple context-free grammar.  By
convention, the left-hand-side of the first production is the
<a name="start_symbol_index_term" /><span class="termdef">start-symbol</span> of the grammar, typically <tt class="doctest"><span class="pre">S</span></tt>, and all
well-formed trees must have this symbol as their root label. In
NLTK, context-free grammars are defined in the <tt class="doctest"><span class="pre">nltk.grammar</span></tt>
module.  In <a class="reference internal" href="#code-cfg1">3.1</a> we define a grammar and show how to parse a
simple sentence admitted by the grammar.</p>
<span class="target" id="code-cfg1"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
grammar1 = nltk.CFG.fromstring(<span class="pysrc-string">&quot;&quot;&quot;</span>
<span class="pysrc-string">  S -&gt; NP VP</span>
<span class="pysrc-string">  VP -&gt; V NP | V NP PP</span>
<span class="pysrc-string">  PP -&gt; P NP</span>
<span class="pysrc-string">  V -&gt; &quot;saw&quot; | &quot;ate&quot; | &quot;walked&quot;</span>
<span class="pysrc-string">  NP -&gt; &quot;John&quot; | &quot;Mary&quot; | &quot;Bob&quot; | Det N | Det N PP</span>
<span class="pysrc-string">  Det -&gt; &quot;a&quot; | &quot;an&quot; | &quot;the&quot; | &quot;my&quot;</span>
<span class="pysrc-string">  N -&gt; &quot;man&quot; | &quot;dog&quot; | &quot;cat&quot; | &quot;telescope&quot; | &quot;park&quot;</span>
<span class="pysrc-string">  P -&gt; &quot;in&quot; | &quot;on&quot; | &quot;by&quot; | &quot;with&quot;</span>
<span class="pysrc-string">  &quot;&quot;&quot;</span>)</pre>
</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sent = <span class="pysrc-string">&quot;Mary saw Bob&quot;</span>.split()
<span class="pysrc-prompt">&gt;&gt;&gt; </span>rd_parser = nltk.RecursiveDescentParser(grammar1)
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">for</span> tree <span class="pysrc-keyword">in</span> rd_parser.parse(sent):
<span class="pysrc-more">... </span>     <span class="pysrc-keyword">print</span>(tree)
<span class="pysrc-output">(S (NP Mary) (VP (V saw) (NP Bob)))</span></pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_cfg1.py" type="text/x-python"><span class="caption-label">Example 3.1 (code_cfg1.py)</span></a>: <span class="caption-label">Figure 3.1</span>: A Simple Context-Free Grammar</td></tr></p>
</table></div>
<p>The grammar in <a class="reference internal" href="#code-cfg1">3.1</a> contains productions involving various syntactic categories,
as laid out in <a class="reference internal" href="#tab-syncat">3.1</a>.</p>
<span class="target" id="tab-syncat"></span><table border="1" class="docutils" id="tab-syncat">
<colgroup>
<col width="13%" />
<col width="42%" />
<col width="46%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Symbol</th>
<th class="head">Meaning</th>
<th class="head">Example</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>S</td>
<td>sentence</td>
<td><span class="example">the man walked</span></td>
</tr>
<tr><td>NP</td>
<td>noun phrase</td>
<td><span class="example">a dog</span></td>
</tr>
<tr><td>VP</td>
<td>verb phrase</td>
<td><span class="example">saw a park</span></td>
</tr>
<tr><td>PP</td>
<td>prepositional phrase</td>
<td><span class="example">with a telescope</span></td>
</tr>
<tr><td>Det</td>
<td>determiner</td>
<td><span class="example">the</span></td>
</tr>
<tr><td>N</td>
<td>noun</td>
<td><span class="example">dog</span></td>
</tr>
<tr><td>V</td>
<td>verb</td>
<td><span class="example">walked</span></td>
</tr>
<tr><td>P</td>
<td>preposition</td>
<td><span class="example">in</span></td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 3.1</span>: <p>Syntactic Categories</p>
</p>
</table>
<p>A production like <tt class="doctest"><span class="pre">VP -&gt; V NP | V NP PP</span></tt> has a disjunction on the
righthand side, shown by the <tt class="doctest"><span class="pre">|</span></tt> and is an abbreviation for the two productions
<tt class="doctest"><span class="pre">VP -&gt; V NP</span></tt> and <tt class="doctest"><span class="pre">VP -&gt; V NP PP</span></tt>.</p>
<span class="target" id="fig-parse-rdparsewindow"></span><div class="figure" id="fig-parse-rdparsewindow">
<img alt="../images/parse_rdparsewindow.png" src="../images/parse_rdparsewindow.png" style="width: 475.0px; height: 464.0px;" />
<p class="caption"><span class="caption-label">Figure 3.2</span>: Recursive Descent Parser Demo: This tool allows you to watch the operation of
a recursive descent parser as it grows the parse tree and matches it against
the input words.</p>
</div>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Try developing a simple grammar of your own, using the
recursive descent parser application, <tt class="doctest"><span class="pre">nltk.app.rdparser()</span></tt>,
shown in <a class="reference internal" href="#fig-parse-rdparsewindow">3.2</a>.
It comes already loaded with a sample grammar, but you can
edit this as you please (using the <tt class="doctest"><span class="pre">Edit</span></tt> menu).
Change the grammar, and the sentence to be parsed, and
run the parser using the <em>autostep</em> button.</p>
</div>
<p>If we parse the sentence <cite>The dog saw a man in the park</cite> using
the grammar shown in <a class="reference internal" href="#code-cfg1">3.1</a>, we end up with two trees, similar to
those we saw for <a class="reference internal" href="#ex-elephant">(3b)</a>:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(9)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td><img alt="tree_images/ch08-tree-4.png" class="align-top" src="tree_images/ch08-tree-4.png" style="width: 336.0px; height: 225.0px;" /></td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td><img alt="tree_images/ch08-tree-5.png" class="align-top" src="tree_images/ch08-tree-5.png" style="width: 336.0px; height: 265.0px;" /></td></tr></table></p>
</td></tr></table></p>
<p>Since our grammar licenses two trees for this sentence, the sentence is
said to be <a name="structurally_ambiguous_index_term" /><span class="termdef">structurally ambiguous</span>.  The ambiguity in question is called
a <a name="prepositional_phrase_attachment_ambiguity_index_term" /><span class="term">prepositional phrase attachment ambiguity</span>, as we saw earlier in this chapter.
As you may recall, it is an ambiguity about attachment since the
<tt class="doctest"><span class="pre">PP</span></tt> <span class="example">in the park</span> needs to be attached to one of two places
in the tree: either as a child of <tt class="doctest"><span class="pre">VP</span></tt> or else as a child of <tt class="doctest"><span class="pre">NP</span></tt>.
When the <tt class="doctest"><span class="pre">PP</span></tt> is attached to <tt class="doctest"><span class="pre">VP</span></tt>, the intended interpretation
is that the seeing event happened
in the park.  However, if the <tt class="doctest"><span class="pre">PP</span></tt> is attached to <tt class="doctest"><span class="pre">NP</span></tt>,
then it was the man who was in the park, and the agent of the seeing (the dog)
might have been sitting on the balcony of an apartment overlooking the
park.</p>
</div>
<div class="section" id="writing-your-own-grammars">
<h2>3.2&nbsp;&nbsp;&nbsp;Writing Your Own Grammars</h2>
<p>If you are interested in experimenting with writing CFGs, you will
find it helpful to create and edit your grammar in a text
file, say <tt class="doctest"><span class="pre">mygrammar.cfg</span></tt>. You can then load it into NLTK and
parse with it as follows:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>grammar1 = nltk.data.load(<span class="pysrc-string">'file:mygrammar.cfg'</span>)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sent = <span class="pysrc-string">&quot;Mary saw Bob&quot;</span>.split()
<span class="pysrc-prompt">&gt;&gt;&gt; </span>rd_parser = nltk.RecursiveDescentParser(grammar1)
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">for</span> tree <span class="pysrc-keyword">in</span> rd_parser.parse(sent):
<span class="pysrc-more">... </span>     <span class="pysrc-keyword">print</span>(tree)</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Make sure that you put a <tt class="doctest"><span class="pre">.cfg</span></tt> suffix on the filename, and that
there are no spaces in the string <tt class="doctest"><span class="pre"><span class="pysrc-string">'file:mygrammar.cfg'</span></span></tt>. If the
command <tt class="doctest"><span class="pre"><span class="pysrc-keyword">print</span>(tree)</span></tt> produces no output, this is probably because
your sentence <tt class="doctest"><span class="pre">sent</span></tt> is not admitted by your grammar. In this case,
call the parser with tracing set to be on: <tt class="doctest"><span class="pre">rd_parser =
nltk.RecursiveDescentParser(grammar1, trace=2)</span></tt>. You can also check
what productions are currently in the grammar with the command <tt class="doctest"><span class="pre"><span class="pysrc-keyword">for</span> p
<span class="pysrc-keyword">in</span> grammar1.productions(): <span class="pysrc-keyword">print</span>(p)</span></tt>.</p>
<p>When you write CFGs for parsing in NLTK, you cannot combine
grammatical categories with lexical items on the righthand side of the
same production. Thus, a production such as <tt class="doctest"><span class="pre">PP -&gt; <span class="pysrc-string">'of'</span> NP</span></tt> is disallowed. In
addition, you are not permitted to place multi-word lexical items on the
righthand side of a production. So rather than writing <tt class="doctest"><span class="pre">NP -&gt; <span class="pysrc-string">'New</span>
<span class="pysrc-string">York'</span></span></tt>, you have to resort to something like <tt class="doctest"><span class="pre">NP -&gt; <span class="pysrc-string">'New_York'</span></span></tt>
instead.</p>
</div>
<div class="section" id="recursion-in-syntactic-structure">
<h2>3.3&nbsp;&nbsp;&nbsp;Recursion in Syntactic Structure</h2>
<p>A grammar is said to be <a name="recursive_index_term" /><span class="termdef">recursive</span> if a category occurring on the left hand
side of a production also appears on
the righthand side of a production, as illustrated in <a class="reference internal" href="#code-cfg2">3.3</a>.
The production <tt class="doctest"><span class="pre">Nom -&gt; Adj Nom</span></tt> (where <tt class="doctest"><span class="pre">Nom</span></tt> is the
category of nominals) involves direct recursion on the category
<tt class="doctest"><span class="pre">Nom</span></tt>, whereas indirect recursion on <tt class="doctest"><span class="pre">S</span></tt> arises from the
combination of two productions, namely <tt class="doctest"><span class="pre">S -&gt; NP VP</span></tt> and <tt class="doctest"><span class="pre">VP -&gt; V S</span></tt>.</p>
<span class="target" id="code-cfg2"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
grammar2 = nltk.CFG.fromstring(<span class="pysrc-string">&quot;&quot;&quot;</span>
<span class="pysrc-string">  S  -&gt; NP VP</span>
<span class="pysrc-string">  NP -&gt; Det Nom | PropN</span>
<span class="pysrc-string">  Nom -&gt; Adj Nom | N</span>
<span class="pysrc-string">  VP -&gt; V Adj | V NP | V S | V NP PP</span>
<span class="pysrc-string">  PP -&gt; P NP</span>
<span class="pysrc-string">  PropN -&gt; 'Buster' | 'Chatterer' | 'Joe'</span>
<span class="pysrc-string">  Det -&gt; 'the' | 'a'</span>
<span class="pysrc-string">  N -&gt; 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'</span>
<span class="pysrc-string">  Adj  -&gt; 'angry' | 'frightened' |  'little' | 'tall'</span>
<span class="pysrc-string">  V -&gt;  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'</span>
<span class="pysrc-string">  P -&gt; 'on'</span>
<span class="pysrc-string">  &quot;&quot;&quot;</span>)</pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_cfg2.py" type="text/x-python"><span class="caption-label">Example 3.3 (code_cfg2.py)</span></a>: <span class="caption-label">Figure 3.3</span>: A Recursive Context-Free Grammar</td></tr></p>
</table></div>
<p>To see how recursion arises from this grammar, consider the following
trees.  <a class="reference internal" href="#ex-recnominals">(10a)</a> involves nested nominal phrases,
while <a class="reference internal" href="#ex-recsentences">(10b)</a> contains nested sentences.</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(10)</td><td width="15"></td><td><span class="target" id="ex-recnominals"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td><img alt="tree_images/ch08-tree-6.png" class="align-top" src="tree_images/ch08-tree-6.png" style="width: 454.0px; height: 265.0px;" /></td></tr></table></p>
<span class="target" id="ex-recsentences"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td><img alt="tree_images/ch08-tree-7.png" class="align-top" src="tree_images/ch08-tree-7.png" style="width: 440.0px; height: 346.0px;" /></td></tr></table></p>
</td></tr></table></p>
<p>We've only illustrated two levels of recursion here, but there's
no upper limit on the depth.  You can experiment with parsing
sentences that involve more deeply nested structures.
Beware that the <tt class="doctest"><span class="pre">RecursiveDescentParser</span></tt> is unable to handle
<a name="left_recursive_index_term" /><span class="termdef">left-recursive</span> productions of the form <tt class="doctest"><span class="pre">X -&gt; X Y</span></tt>; we will
return to this in <a class="reference internal" href="#sec-parsing">4</a>.</p>
</div>
</div>
<div class="section" id="parsing-with-context-free-grammar">
<span id="sec-parsing"></span><h1>4&nbsp;&nbsp;&nbsp;Parsing With Context Free Grammar</h1>
<!-- >>> grammar1 = nltk.CFG.fromstring("""
...     S -> NP VP
...     VP -> V NP | V NP PP
...     PP -> P NP
...     V -> "saw" | "ate" | "walked"
...     NP -> "John" | "Mary" | "Bob" | Det N | Det N PP
...     Det -> "a" | "an" | "the" | "my"
...     N -> "man" | "dog" | "cat" | "telescope" | "park"
...     P -> "in" | "on" | "by" | "with"
...     """)
>>> groucho_grammar = nltk.CFG.fromstring("""
... S -> NP VP
... PP -> P NP
... NP -> Det N | Det N PP | 'I'
... VP -> V NP | VP PP
... Det -> 'an' | 'my'
... N -> 'elephant' | 'pajamas'
... V -> 'shot'
... P -> 'in'
... """) -->
<p>A <a name="parser_index_term" /><span class="termdef">parser</span> processes input sentences according to the
productions of a grammar, and builds one or more
constituent structures that conform to the grammar.
A grammar is a declarative specification of well-formedness &#8212;
it is actually just a string, not a program.
A parser is a procedural interpretation of the grammar.
It searches through the space of trees licensed by a grammar
to find one that has the required sentence along its fringe.</p>
<!-- XXX does the following read as though our audience does not consist
of people who want to do the tasks described in this chapter? -->
<p>A parser permits a grammar to be evaluated against
a collection of test sentences, helping linguists
to discover mistakes in their grammatical analysis.
A parser can serve as a model of psycholinguistic processing,
helping to explain the difficulties that humans have with processing
certain syntactic constructions.
Many natural language applications involve parsing at some point;
for example, we would expect the natural language questions
submitted to a question-answering system to undergo parsing as an initial step.</p>
<p>In this section we see two simple parsing algorithms,
a top-down method called recursive descent parsing,
and a bottom-up method called shift-reduce parsing.
We also see some more sophisticated algorithms,
a top-down method with bottom-up filtering called
left-corner parsing, and a dynamic programming
technique called chart parsing.</p>
<div class="section" id="recursive-descent-parsing">
<h2>4.1&nbsp;&nbsp;&nbsp;Recursive Descent Parsing</h2>
<p>The simplest kind of parser interprets a grammar as a specification
of how to break a high-level goal into several lower-level subgoals.
The top-level goal is to find an <tt class="doctest"><span class="pre">S</span></tt>.  The <tt class="doctest"><span class="pre">S</span></tt> &#8594; <tt class="doctest"><span class="pre">NP VP</span></tt>
production permits the parser to replace this goal with two subgoals:
find an <tt class="doctest"><span class="pre">NP</span></tt>, then find a <tt class="doctest"><span class="pre">VP</span></tt>.  Each of these subgoals can be
replaced in turn by sub-sub-goals, using productions that have <tt class="doctest"><span class="pre">NP</span></tt>
and <tt class="doctest"><span class="pre">VP</span></tt> on their left-hand side.  Eventually, this expansion
process leads to subgoals such as: find the word <span class="example">telescope</span>.  Such
subgoals can be directly compared against the input sequence, and
succeed if the next word is matched.  If there is no match the parser
must back up and try a different alternative.</p>
<p>The recursive descent parser builds a parse tree during the above
process.  With the initial goal (find an <tt class="doctest"><span class="pre">S</span></tt>), the <tt class="doctest"><span class="pre">S</span></tt> root node
is created.  As the above process recursively expands its goals using
the productions of the grammar, the parse tree is extended downwards
(hence the name <em>recursive descent</em>).  We can see this in action using
the graphical demonstration <tt class="doctest"><span class="pre">nltk.app.rdparser()</span></tt>.
Six stages of the execution of this parser are shown in <a class="reference internal" href="#fig-rdparser1-6">4.1</a>.</p>
<span class="target" id="fig-rdparser1-6"></span><div class="figure" id="fig-rdparser1-6">
<img alt="../images/rdparser1-6.png" src="../images/rdparser1-6.png" style="width: 1062.5px; height: 508.5px;" />
<p class="caption"><span class="caption-label">Figure 4.1</span>: Six Stages of a Recursive Descent Parser: the parser begins with a
tree consisting of the node <tt class="doctest"><span class="pre">S</span></tt>; at each stage it consults the grammar
to find a production that can be used to enlarge the tree; when
a lexical production is encountered, its word is compared against the input;
after a complete parse has been found, the parser backtracks to look for
more parses.</p>
</div>
<p>During this process, the parser is often forced to choose between several
possible productions.  For example, in going from step 3 to step 4, it
tries to find productions with <tt class="doctest"><span class="pre">N</span></tt> on the left-hand side.  The
first of these is <tt class="doctest"><span class="pre">N</span></tt> &#8594; <span class="example">man</span>.  When this does not work
it <a name="backtracks_index_term" /><span class="term">backtracks</span>, and tries other <tt class="doctest"><span class="pre">N</span></tt> productions in order, until it
gets to <tt class="doctest"><span class="pre">N</span></tt> &#8594; <span class="example">dog</span>, which matches the next word in the
input sentence.  Much later, as shown in step 5, it finds a complete
parse.  This is a tree that covers the entire sentence, without any
dangling edges.  Once a parse has been found, we can get the parser to
look for additional parses.  Again it will backtrack and explore other
choices of production in case any of them result in a parse.</p>
<p>NLTK provides a recursive descent parser:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>rd_parser = nltk.RecursiveDescentParser(grammar1)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sent = <span class="pysrc-string">'Mary saw a dog'</span>.split()
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">for</span> tree <span class="pysrc-keyword">in</span> rd_parser.parse(sent):
<span class="pysrc-more">... </span>    <span class="pysrc-keyword">print</span>(tree)
<span class="pysrc-output">(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><tt class="doctest"><span class="pre">RecursiveDescentParser()</span></tt> takes an optional parameter <tt class="doctest"><span class="pre">trace</span></tt>.
If <tt class="doctest"><span class="pre">trace</span></tt> is greater than zero, then the parser will report the steps
that it takes as it parses a text.</p>
</div>
<p>Recursive descent parsing has three key shortcomings.  First,
left-recursive productions like <tt class="doctest"><span class="pre">NP -&gt; NP PP</span></tt> send it
into an infinite loop.  Second, the parser wastes a lot of time
considering words and structures that do not correspond to the input
sentence.  Third, the backtracking process may discard parsed
constituents that will need to be rebuilt again later.  For example,
backtracking over <tt class="doctest"><span class="pre">VP -&gt; V NP</span></tt> will discard the subtree
created for the <tt class="doctest"><span class="pre">NP</span></tt>.  If the parser then proceeds with
<tt class="doctest"><span class="pre">VP -&gt; V NP PP</span></tt>, then the <tt class="doctest"><span class="pre">NP</span></tt> subtree must be created all
over again.</p>
<p>Recursive descent parsing is a kind of <a name="top_down_parsing_index_term" /><span class="termdef">top-down parsing</span>.
Top-down parsers use a grammar to <em>predict</em> what the input will be,
before inspecting the input!  However, since the input is available to
the parser all along, it would be more sensible to consider the input
sentence from the very beginning.  This approach is called
<a name="bottom_up_parsing_index_term" /><span class="termdef">bottom-up parsing</span>, and we will see an example in the next section.</p>
</div>
<div class="section" id="shift-reduce-parsing">
<h2>4.2&nbsp;&nbsp;&nbsp;Shift-Reduce Parsing</h2>
<p>A simple kind of bottom-up parser is the <a name="shift_reduce_parser_index_term" /><span class="termdef">shift-reduce parser</span>.
In common with all bottom-up parsers, a shift-reduce
parser tries to find sequences of words and phrases that correspond
to the <em>right hand</em> side of a grammar production, and replace them
with the left-hand side, until the whole sentence is reduced to
an <tt class="doctest"><span class="pre">S</span></tt>.</p>
<!-- XXX earlier section no longer talks about stacks.  Concepts of
pushing and popping will need to be explained somewhere. -->
<p>The shift-reduce parser repeatedly pushes the next input word onto a
stack (<a class="reference external" href="ch04.html#sec-back-to-the-basics">4.1</a>); this is the <a name="shift_index_term" /><span class="termdef">shift</span> operation.
If the top <em>n</em> items on the stack match
the <em>n</em> items on the right hand side of some production,
then they are all popped off the stack, and the item on the left-hand
side of the production is pushed on the stack.  This replacement of
the top <em>n</em> items with a single item is the <a name="reduce_index_term" /><span class="termdef">reduce</span> operation.
This operation may only be applied to the top of the stack;
reducing items lower in the stack must be done before later items are
pushed onto the stack.  The parser finishes when all the input is
consumed and there is only one item remaining on the stack, a parse
tree with an <tt class="doctest"><span class="pre">S</span></tt> node as its root.
The shift-reduce parser builds a parse tree during the above process.
Each time it pops <em>n</em> items off the stack it combines them into
a partial parse tree, and pushes this back on the stack.
We can see the shift-reduce parsing algorithm in action using the
graphical demonstration <tt class="doctest"><span class="pre">nltk.app.srparser()</span></tt>.
Six stages of the execution of this parser are shown in <a class="reference internal" href="#fig-srparser1-6">4.2</a>.</p>
<span class="target" id="fig-srparser1-6"></span><div class="figure" id="fig-srparser1-6">
<img alt="../images/srparser1-6.png" src="../images/srparser1-6.png" style="width: 1010.5px; height: 598.0px;" />
<p class="caption"><span class="caption-label">Figure 4.2</span>: Six Stages of a Shift-Reduce Parser: the parser begins by shifting the
first input word onto its stack; once the top items on the stack match
the right hand side of a grammar production, they can be replaced with
the left hand side of that production; the parser succeeds once all input
is consumed and one <tt class="doctest"><span class="pre">S</span></tt> item remains on the stack.</p>
</div>
<p>NLTK provides <tt class="doctest"><span class="pre">ShiftReduceParser()</span></tt>, a simple
implementation of a shift-reduce parser.  This parser does not
implement any backtracking, so it is not guaranteed to find a parse
for a text, even if one exists.  Furthermore, it will only find at
most one parse, even if more parses exist.  We can provide an
optional <tt class="doctest"><span class="pre">trace</span></tt> parameter that controls how verbosely the
parser reports the steps that it takes as it parses a text:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sr_parser = nltk.ShiftReduceParser(grammar1)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sent = <span class="pysrc-string">'Mary saw a dog'</span>.split()
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">for</span> tree <span class="pysrc-keyword">in</span> sr_parser.parse(sent):
<span class="pysrc-more">... </span>    <span class="pysrc-keyword">print</span>(tree)
<span class="pysrc-output">  (S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Run the above parser in tracing mode to see the sequence of shift and reduce
operations, using <tt class="doctest"><span class="pre">sr_parse = nltk.ShiftReduceParser(grammar1, trace=2)</span></tt></p>
</div>
<p>A shift-reduce parser can reach a dead end and fail to find any parse,
even if the input sentence is well-formed according to the grammar.
When this happens, no input remains, and the stack contains items
which cannot be reduced to an <tt class="doctest"><span class="pre">S</span></tt>.  The problem arises because
there are choices made earlier that cannot be undone by the parser
(although users of the graphical demonstration can undo their choices).
There are two kinds of choices to be made by the parser:
(a) which reduction to do when more than one is possible
(b) whether to shift or reduce when either action is possible.</p>
<p>A shift-reduce parser may be extended to implement policies for resolving such
conflicts.  For example, it may address shift-reduce conflicts by
shifting only when no reductions are possible, and it may address
reduce-reduce conflicts by favoring the reduction operation that removes
the most items from the stack.  (A generalization of shift-reduce
parser, a &quot;lookahead LR parser&quot;, is commonly used in programming
language compilers.)</p>
<p>The advantage of shift-reduce parsers over recursive descent parsers
is that they only build structure that corresponds to the words in the
input.  Furthermore, they only build each sub-structure once,
e.g. <tt class="doctest"><span class="pre">NP(Det(the), N(man))</span></tt> is only built and pushed onto the stack
a single time, regardless of whether it will later be used by the
<tt class="doctest"><span class="pre">VP -&gt; V NP PP</span></tt> reduction or the <tt class="doctest"><span class="pre">NP -&gt; NP PP</span></tt> reduction.</p>
</div>
<div class="section" id="the-left-corner-parser">
<h2>4.3&nbsp;&nbsp;&nbsp;The Left-Corner Parser</h2>
<p>One of the problems with the recursive descent parser is that it
goes into an infinite loop when it encounters a left-recursive production.
This is because it applies the grammar
productions blindly, without considering the actual input sentence.
A left-corner parser is a hybrid between the bottom-up and top-down
approaches we have seen.</p>
<p>Grammar <tt class="doctest"><span class="pre">grammar1</span></tt> allows us to produce the following parse of <span class="example">John saw
Mary</span>:</p>
<span class="target" id="ex-jmtree"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(11)</td><td width="15"></td><td><img alt="tree_images/ch08-tree-8.png" class="align-top" src="tree_images/ch08-tree-8.png" style="width: 140.0px; height: 144.0px;" /></td></tr></table></p>
<p>Recall that the grammar (defined in <a class="reference internal" href="#code-cfg2">3.3</a>) has the following productions for expanding <tt class="doctest"><span class="pre">NP</span></tt>:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(12)</td><td width="15"></td><td><span class="target" id="ex-r1"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td><tt class="doctest"><span class="pre">NP -&gt; Det N</span></tt></td></tr></table></p>
<span class="target" id="ex-r2"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td><tt class="doctest"><span class="pre">NP -&gt; Det N PP</span></tt></td></tr></table></p>
<span class="target" id="ex-r3"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td><tt class="doctest"><span class="pre">NP -&gt; <span class="pysrc-string">&quot;John&quot;</span> | <span class="pysrc-string">&quot;Mary&quot;</span> | <span class="pysrc-string">&quot;Bob&quot;</span></span></tt></td></tr></table></p>
</td></tr></table></p>
<!-- XXX Following notation with DoubleRightArrow wrongly assumes
this relation has been defined. -->
<p>Suppose we ask you to first look at tree <a class="reference internal" href="#ex-jmtree">(11)</a>, and then decide
which of the <tt class="doctest"><span class="pre">NP</span></tt> productions you'd want a recursive descent parser to
apply first &#8212; obviously, <a class="reference internal" href="#ex-r3">(12c)</a> is the right choice! How do you
know that it would be pointless to apply <a class="reference internal" href="#ex-r1">(12a)</a> or <a class="reference internal" href="#ex-r2">(12b)</a> instead? Because
neither of these productions will derive a sequence whose first word is
<span class="example">John</span>.  That is, we can easily tell that in a successful
parse of <span class="example">John saw Mary</span>, the parser has to expand <tt class="doctest"><span class="pre">NP</span></tt> in
such a way that <tt class="doctest"><span class="pre">NP</span></tt> derives the sequence <span class="example">John</span> &#945;. More
generally, we say that a category <span class="math">B</span> is a <a name="left_corner_index_term" /><span class="termdef">left-corner</span> of
a tree rooted in <span class="math">A</span> if  <span class="math">A</span> &#8658;*
<span class="math">B</span> &#945;.</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(13)</td><td width="15"></td><td><img alt="tree_images/ch08-tree-9.png" class="align-top" src="tree_images/ch08-tree-9.png" style="width: 49.0px; height: 64.0px;" /></td></tr></table></p>
<p>A <a name="left_corner_parser_index_term" /><span class="termdef">left-corner parser</span> is a top-down parser with bottom-up filtering.
Unlike an ordinary recursive descent parser, it does not get trapped
in left recursive productions.
Before starting its work, a left-corner parser preprocesses the
context-free grammar to build a table where each row contains two
cells, the first holding a non-terminal, and the second holding the
collection of possible left corners of that non-terminal. <a class="reference internal" href="#tab-lc">4.1</a>
illustrates this for the grammar from <tt class="doctest"><span class="pre">grammar2</span></tt>.</p>
<span class="target" id="tab-lc"></span><table border="1" class="docutils" id="tab-lc">
<colgroup>
<col width="22%" />
<col width="78%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Category</th>
<th class="head">Left-Corners (pre-terminals)</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>S</td>
<td>NP</td>
</tr>
<tr><td>NP</td>
<td>Det, PropN</td>
</tr>
<tr><td>VP</td>
<td>V</td>
</tr>
<tr><td>PP</td>
<td>P</td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 4.1</span>: <p>Left-Corners in <tt class="doctest"><span class="pre">grammar2</span></tt></p>
</p>
</table>
<p>Each time a production is considered by the parser, it checks that the
next input word is compatible with at least one of the pre-terminal
categories in the left-corner table.</p>
<!-- XXX explain how this effects the action of the parser, and why this solves the problem -->
</div>
<div class="section" id="well-formed-substring-tables">
<h2>4.4&nbsp;&nbsp;&nbsp;Well-Formed Substring Tables</h2>
<p>The simple parsers discussed above suffer from limitations in
both completeness and efficiency. In order to remedy these, we will
apply the algorithm design technique of <a name="dynamic_programming_index_term" /><span class="term">dynamic programming</span> to
the parsing problem.  As we saw in <a class="reference external" href="ch04.html#sec-algorithm-design">4.7</a>,
dynamic programming stores intermediate results and re-uses them when
appropriate, achieving significant efficiency gains. This technique
can be applied to syntactic parsing, allowing us to store
partial solutions to the parsing task and then look them up as
necessary in order to efficiently arrive at a complete solution.
This approach to parsing is known as <a name="chart_parsing_index_term" /><span class="termdef">chart parsing</span>.  We introduce
the main idea in this section; see the online materials available for
this chapter for more implementation details.</p>
<p>Dynamic programming allows us to build the <tt class="doctest"><span class="pre">PP</span></tt> <span class="example">in my pajamas</span>
just once.  The first time we build it we save it in a table, then we look it
up when we need to use it as a subconstituent of either the object <tt class="doctest"><span class="pre">NP</span></tt> or
the higher <tt class="doctest"><span class="pre">VP</span></tt>. This table is known as a
<a name="well_formed_substring_table_index_term" /><span class="termdef">well-formed substring table</span>, or WFST for short.
(The term &quot;substring&quot; refers to a contiguous sequence of words within a sentence.)
We will show how to construct the WFST bottom-up so as to systematically record
what syntactic constituents have been found.</p>
<p>Let's set our input to be the sentence in <a class="reference internal" href="#ex-marx-elephant">(2)</a>.  The numerically specified
spans of the WFST are reminiscent of Python's slice notation (<a class="reference external" href="ch03.html#sec-strings">3.2</a>).  Another
way to think about the data structure is shown in <a class="reference internal" href="#fig-chart-positions1">4.3</a>, a data
structure known as a <a name="chart_index_term" /><span class="termdef">chart</span>.</p>
<span class="target" id="fig-chart-positions1"></span><div class="figure" id="fig-chart-positions1">
<img alt="../images/chart_positions1.png" src="../images/chart_positions1.png" style="width: 129.75px; height: 11.0px;" />
<p class="caption"><span class="caption-label">Figure 4.3</span>: The Chart Data Structure: words are the edge labels of a linear graph structure.</p>
</div>
<p>In a WFST, we record the position of the words
by filling in cells in a triangular matrix:
the vertical axis will denote the start position of a substring,
while the horizontal axis will denote the end position
(thus <span class="example">shot</span> will appear in the cell with coordinates (1, 2)).
To simplify this presentation, we will assume each word has a unique
lexical category, and we will store this (not the word) in the matrix.
So cell (1, 2) will contain the entry <tt class="doctest"><span class="pre">V</span></tt>.
More generally, if our input string is
<cite>a</cite><sub>0</sub><cite>a</cite><sub>1</sub> ... <cite>a</cite><sub>n</sub>, and our grammar
contains a production of the form <em>A</em> &#8594; <cite>a</cite><sub>i</sub>, then we add <em>A</em> to
the cell (<cite>i</cite>, <a href="#id1"><span class="problematic" id="id2">`</span></a>i`+1).</p>
<div class="system-message" id="id1">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">ch08.rst2</tt>, line 900); <em><a href="#id2">backlink</a></em></p>
Inline interpreted text or phrase reference start-string without end-string.</div>
<p>So, for every word in <tt class="doctest"><span class="pre">text</span></tt>, we can look up in our grammar what
category it belongs to.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>text = [<span class="pysrc-string">'I'</span>, <span class="pysrc-string">'shot'</span>, <span class="pysrc-string">'an'</span>, <span class="pysrc-string">'elephant'</span>, <span class="pysrc-string">'in'</span>, <span class="pysrc-string">'my'</span>, <span class="pysrc-string">'pajamas'</span>]
<span class="pysrc-prompt">&gt;&gt;&gt; </span>groucho_grammar.productions(rhs=text[1])
<span class="pysrc-output">[V -&gt; 'shot']</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>For our WFST, we create an <span class="math">(n-1)</span> &#215; <span class="math">(n-1)</span> matrix
as a list of lists in Python, and initialize it
with the lexical categories of each token, in the <tt class="doctest"><span class="pre">init_wfst()</span></tt>
function in <a class="reference internal" href="#code-wfst">4.4</a>.  We also define a utility function <tt class="doctest"><span class="pre">display()</span></tt>
to pretty-print the WFST for us.
As expected, there is a <tt class="doctest"><span class="pre">V</span></tt> in cell (1, 2).</p>
<span 